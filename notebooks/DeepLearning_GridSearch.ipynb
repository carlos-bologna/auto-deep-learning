{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Packages\n",
    "from core.parameters import *\n",
    "from core.net_list import NET_LIST\n",
    "from core.scheduler_list import SCHEDULER_LIST\n",
    "from core.optimizer_list import OPTIMIZER_LIST\n",
    "from core.loss_list import LOSS_LIST\n",
    "from nets.ResNet50Attention import ResNet50Attention\n",
    "from common.myfunctions import plot_confusion_matrix\n",
    "from common.customloss import QuadraticKappa, WeightedMultiLabelLogLoss, WeightedMultiLabelFocalLogLoss\n",
    "import common.weights_initialization as w_init\n",
    "\n",
    "# Base Packages\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#import pydicom\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Torchvision Packages\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.models import densenet121, vgg16, resnet50, inception_v3\n",
    "\n",
    "# Miscellaneous Packages\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from skimage import io, transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from sklearn.utils import class_weight\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True , Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): #GPU\n",
    "    is_cuda = True\n",
    "    cuda_list = ','.join([str(c) for c in CUDA_DEVICES])\n",
    "    device = torch.device(\"cuda:{}\".format(cuda_list))\n",
    "    \n",
    "else: #CPU\n",
    "    is_cuda = False\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set seed for CUDA (all GPU)    \n",
    "#torch.cuda.manual_seed_all(SEED)    \n",
    "    \n",
    "print('Cuda:', is_cuda, ', Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, img_folder, img_ext='png', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (dataframe): Dataframe with images ID.\n",
    "            y (dataframe): Dataframe with labels annotations.\n",
    "            img_folder (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.img_folder = img_folder\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_folder, self.X.iloc[idx, 0] + '.' + self.img_ext)\n",
    "        \n",
    "        image = np.load(img_name)\n",
    "        \n",
    "        label = self.y.iloc[idx].to_numpy()\n",
    "        \n",
    "        if self.transform:\n",
    "       \n",
    "           image = self.transform(TF.to_pil_image(image))\n",
    "\n",
    "        return (image,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f59b0ec7c18>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_csv(TRAIN_LABELS)\n",
    "\n",
    "# Delete Blacklist\n",
    "data = data.loc[~data[ID_COLUMN].isin(BLACK_LIST_ID)]\n",
    "\n",
    "# Plot All Labels\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "data[LABEL_COLUMN].sum().plot.bar()\n",
    "\n",
    "# Sample Dataset\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    data = data.sample(frac = SAMPLE_FRAC)\n",
    "\n",
    "# Plot Sample Labels\n",
    "plt.subplot(1,2,2)\n",
    "data[LABEL_COLUMN].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc Classes Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_CLASSES > 1:\n",
    "    \n",
    "    distrib_freq = data[LABEL_COLUMN].sum().to_numpy()\n",
    "    \n",
    "    w_classes = distrib_freq.sum() / (NUM_CLASSES * distrib_freq)\n",
    "    \n",
    "    for l in LOSSES:\n",
    "        if 'weight' in LOSS_LIST[l]:\n",
    "            LOSS_LIST[l]['weight'] = torch.from_numpy(w_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop([*LABEL_COLUMN], axis=1)\n",
    "y = data[LABEL_COLUMN]\n",
    "\n",
    "# Criando o dataframe de treine e teste com base no dataframe anteriormente criado\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = TEST_SPLIT, random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transf = transforms.Compose(TRAIN_AUGMENTATION)\n",
    "test_transf = transforms.Compose(TEST_AUGMENTATION)\n",
    "\n",
    "train_dataset = CustomDataset(X=X_train, \n",
    "                              y=y_train, \n",
    "                              img_folder=TRAIN_DIR,\n",
    "                              img_ext=IMAGE_FORMAT, \n",
    "                              transform=train_transf)\n",
    "\n",
    "test_dataset = CustomDataset(X=X_test, \n",
    "                             y=y_test, \n",
    "                             img_folder=TEST_DIR, \n",
    "                             img_ext=IMAGE_FORMAT,\n",
    "                             transform=test_transf)\n",
    "\n",
    "# Garregando os dados\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Make a dict to pass though train function\n",
    "dataloaders_dict = {'train': train_loader, 'val': test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name, num_classes):\n",
    "    \n",
    "    model_parameters = NET_LIST[model_name]\n",
    "    base_model = model_parameters['base_model']\n",
    "    \n",
    "    if base_model=='densenet121':\n",
    "        \n",
    "        model = densenet121(pretrained=model_parameters['pretrained'])\n",
    "        model.classifier = nn.Linear(1024, num_classes)   \n",
    "            \n",
    "    elif base_model=='densenet121multitask':\n",
    "        \n",
    "        model = densenet121multitask(pretrained=model_parameters['pretrained'])\n",
    "        model.classifier = nn.Linear(1024, num_classes)   \n",
    "        model.aux_classifier = nn.Linear(1024, 1)   \n",
    "            \n",
    "    elif base_model=='vgg16':\n",
    "        \n",
    "        model = vgg16(pretrained=model_parameters['pretrained'])\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes) \n",
    "    \n",
    "    elif base_model=='resnet50':\n",
    "        \n",
    "        model = resnet50(pretrained=model_parameters['pretrained'])\n",
    "        model.fc = nn.Linear(2048, num_classes) \n",
    "        \n",
    "    elif base_model=='ResNet50Attention':\n",
    "        model = ResNet50Attention(num_classes, \n",
    "                                  attention=True, \n",
    "                                  pretrained=model_parameters['pretrained'])\n",
    "        \n",
    "    elif base_model=='ResNet50AttentionMultiTask':\n",
    "        model = ResNet50AttentionMultiTask(num_classes, \n",
    "                                  attention=True, \n",
    "                                  pretrained=model_parameters['pretrained'])\n",
    "        \n",
    "    elif base_model=='inception_v3':\n",
    "        \n",
    "        model = inception_v3(pretrained=model_parameters['pretrained'])\n",
    "        model.fc = nn.Linear(2048, num_classes) \n",
    "        model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "        \n",
    "    elif base_model=='efficientnetb7':\n",
    "        \n",
    "        model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        model._fc = nn.Linear(2560, NUM_CLASSES) \n",
    "        \n",
    "        \n",
    "    # Parallel    \n",
    "    # Obs.: when load model, the DataParallel is already in the model.\n",
    "    if is_cuda & (torch.cuda.device_count() > 1) & (not model_parameters['is_inception']):\n",
    "        \n",
    "        if not CUDA_DEVICES:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model) \n",
    "        else:\n",
    "            print(\"Let's use\", CUDA_DEVICES, \"GPUs!\")\n",
    "            model = nn.DataParallel(model, device_ids = CUDA_DEVICES) # When load checkpoint, the DataParallel is already in the model.\n",
    "    \n",
    "    # Frozen Layers\n",
    "    for name, param in model.named_parameters():\n",
    "        for l in model_parameters['layers_to_frozen']:\n",
    "            if l in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    if LOAD_CHECKPOINT:\n",
    "\n",
    "        # Get lastest model file\n",
    "        list_of_files = glob.glob(MODEL_DIR + f'/{base_model}_*.pt') # * means all if need specific format then *.csv\n",
    "        \n",
    "        if len(list_of_files) > 0:\n",
    "            \n",
    "            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "            print(f'Loading state dict from checkpoint \\n\\t {latest_file}')\n",
    "\n",
    "            model.load_state_dict(torch.load(latest_file, map_location=device))\n",
    "    else:\n",
    "        model.apply(w_init.weight_init) #Custom weight initialization\n",
    "        \n",
    "    if is_cuda:\n",
    "        model = model.to(device)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScheduler(scheduler_name, optimizer):\n",
    "    \n",
    "    if not scheduler_name:\n",
    "        return None\n",
    "\n",
    "    scheduler_parameters = SCHEDULER_LIST[scheduler_name]\n",
    "\n",
    "    if scheduler_parameters['function'] == 'ReduceLROnPlateau':\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                      mode = scheduler_parameters['mode'], \n",
    "                                      factor = scheduler_parameters['factor'], \n",
    "                                      patience = scheduler_parameters['patience'], \n",
    "                                      verbose = scheduler_parameters['verbose'], \n",
    "                                      threshold = scheduler_parameters['threshold'], \n",
    "                                      threshold_mode = scheduler_parameters['threshold_mode'], \n",
    "                                      cooldown = scheduler_parameters['cooldown'], \n",
    "                                      min_lr = scheduler_parameters['min_lr'], \n",
    "                                      eps = scheduler_parameters['eps'])\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimizer(optimizer_name, model):\n",
    "\n",
    "    params_to_update = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "    \n",
    "        if param.requires_grad == True:\n",
    "        \n",
    "            params_to_update.append(param)\n",
    "            \n",
    "            #print(\"\\t\",name)\n",
    "            \n",
    "    opt_parameters = OPTIMIZER_LIST[optimizer_name]\n",
    "\n",
    "    if opt_parameters['function'] == 'Adam':\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params_to_update, \n",
    "                                     lr = opt_parameters['lr'],\n",
    "                                     betas = opt_parameters['betas'],\n",
    "                                     eps = opt_parameters['eps'],\n",
    "                                     weight_decay = opt_parameters['weight_decay'],\n",
    "                                     amsgrad = opt_parameters['amsgrad']\n",
    "                                    )\n",
    "    elif opt_parameters['function'] == 'SGD':\n",
    "        \n",
    "        optimizer = torch.optim.SGD(params_to_update, \n",
    "                                     lr = opt_parameters['lr'],\n",
    "                                     weight_decay = opt_parameters['weight_decay'],\n",
    "                                     momentum = opt_parameters['momentum']\n",
    "                                    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLossFunction(loss_nme):\n",
    "    \n",
    "    loss_parameters = LOSS_LIST[loss_nme]\n",
    "\n",
    "    if loss_parameters['function'] == 'SmoothL1Loss':\n",
    "        criterion = nn.SmoothL1Loss(\n",
    "            reduction = loss_parameters['reduction']\n",
    "        )\n",
    "\n",
    "    elif loss_parameters['function'] == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss(\n",
    "            weight = loss_parameters['weight'],\n",
    "            size_average = loss_parameters['size_average'],\n",
    "            ignore_index = loss_parameters['ignore_index'],\n",
    "            reduce = loss_parameters['reduce'],\n",
    "            reduction = loss_parameters['reduction']\n",
    "        )\n",
    "\n",
    "    elif loss_parameters['function'] == 'NLLLoss':\n",
    "\n",
    "        criterion = nn.NLLLoss(\n",
    "            weight = loss_parameters['weight'],\n",
    "            size_average = loss_parameters['size_average'],\n",
    "            ignore_index = loss_parameters['ignore_index'],\n",
    "            reduce = loss_parameters['reduce'],\n",
    "            reduction = loss_parameters['reduction']\n",
    "        )\n",
    "\n",
    "    elif loss_parameters['function'] == 'QuadraticKappa':\n",
    "        criterion = QuadraticKappa(\n",
    "            n_classes = loss_parameters['n_classes']\n",
    "        )\n",
    "        \n",
    "    elif loss_parameters['function'] == 'WeightedMultiLabelLogLoss':\n",
    "\n",
    "        criterion = WeightedMultiLabelLogLoss(\n",
    "            n_classes = loss_parameters['n_classes'],\n",
    "            weight = loss_parameters['weight']\n",
    "        )\n",
    "    elif loss_parameters['function'] == 'WeightedMultiLabelFocalLogLoss':\n",
    "\n",
    "        criterion = WeightedMultiLabelFocalLogLoss(\n",
    "            n_classes = loss_parameters['n_classes'],\n",
    "            weight = loss_parameters['weight'],\n",
    "            gamma = loss_parameters['gamma']\n",
    "        )\n",
    "        \n",
    "    return criterion\n",
    "\n",
    "def onehot(labels, num_classes):\n",
    "    return torch.zeros(len(labels), num_classes).scatter_(1, labels.unsqueeze(1).cpu(), 1.).cuda()\n",
    "\n",
    "\n",
    "def calcLoss(criterion, loss_name, outputs, labels):\n",
    "    \n",
    "    loss_parameters = LOSS_LIST[loss_name]\n",
    "    last_layer = loss_parameters['last_layer']\n",
    "    \n",
    "    if last_layer == 'softmax':\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        preds_loss = torch.argmax(outputs, 1)\n",
    "        preds_metric = torch.argmax(outputs, 1)\n",
    "        \n",
    "    elif last_layer == 'logsoftmax':\n",
    "        logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        outputs = logsoftmax(outputs)\n",
    "        preds_loss = outputs\n",
    "        preds_metric = torch.argmax(torch.exp(outputs),  1) ### AINDA NÃƒO TESTADO.\n",
    "        \n",
    "        #OBS.: torch.exp(outputs) revert log\n",
    "        \n",
    "    elif last_layer == 'sigmoid':        \n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        preds_loss = outputs > 0.5\n",
    "        preds_metric = torch.argmax(outputs, 1)\n",
    "        \n",
    "    elif last_layer == 'linear':        \n",
    "        preds_loss = outputs\n",
    "        preds_metric = outputs\n",
    "        labels = labels.type(torch.float)\n",
    "\n",
    "    # Transform label from shape 1 to (1, n_classes)\n",
    "    if loss_parameters['onehotlabel']:\n",
    "        labels = onehot(labels, NUM_CLASSES)\n",
    "        \n",
    "    loss = criterion(preds_loss, labels)\n",
    "            \n",
    "    return loss, preds_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMetric(preds, labels):\n",
    "    \n",
    "    if METRIC == 'KAPPA':\n",
    "        preds = np.round(preds)\n",
    "        score = cohen_kappa_score(preds, labels, weights='quadratic')\n",
    "    \n",
    "    elif METRIC == 'ACC':\n",
    "        score = sum(preds == labels)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_name, loss_name, dataloaders, criterion, optimizer, scheduler, num_epochs=25, is_inception=False):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_score = 0.0 if SAVE_BEST == 'metric' else float(\"inf\")\n",
    "    epoch_metric = 0.0\n",
    "    \n",
    "    print(model_name)\n",
    "    print('-' * 100)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "        print('Learning Rate:', lr)\n",
    "        tensorboard.add_scalar('LR', lr, epoch)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_preds = []\n",
    "            running_labels = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        \n",
    "                        loss1, preds = calcLoss(criterion, loss_name, outputs, labels)\n",
    "                        loss2, preds = calcLoss(criterion, loss_name, aux_outputs, labels)\n",
    "                        \n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        \n",
    "                        loss, preds = calcLoss(criterion, loss_name, outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                running_preds = np.append(running_preds, preds.squeeze().cpu().detach().numpy())\n",
    "                running_labels = np.append(running_labels, labels.squeeze().cpu().detach().numpy())\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if METRIC:\n",
    "                epoch_metric = calcMetric(running_preds, running_labels)\n",
    "                tensorboard.add_scalar('{} {}'.format(METRIC, phase), epoch_metric, epoch)\n",
    "            \n",
    "            print('{} Loss: {:.4f} {}: {:.4f}'.format(phase, epoch_loss, METRIC, epoch_metric))\n",
    "            \n",
    "            # Write loss into Tensorboard\n",
    "            tensorboard.add_scalar('Loss {}'.format(phase), epoch_loss, epoch)\n",
    "\n",
    "            # Save the best model\n",
    "            if phase == 'val':\n",
    "                \n",
    "                if scheduler:\n",
    "                    scheduler.step(epoch_loss)\n",
    "                \n",
    "                save_flag = False\n",
    "                \n",
    "                if SAVE_BEST == 'metric' and epoch_metric > best_score:\n",
    "                    \n",
    "                    best_score = epoch_metric\n",
    "                    save_flag = True\n",
    "                    \n",
    "                elif SAVE_BEST == 'loss' and epoch_loss < best_score:\n",
    "                    \n",
    "                    best_score = epoch_loss\n",
    "                    save_flag = True\n",
    "                \n",
    "                if save_flag:\n",
    "                    print('Saving the best model at {}'.format(MODEL_DIR))\n",
    "                    torch.save(model.state_dict(), MODEL_DIR + '/' + model_name + '_' + SAVE_BEST + str(best_score) + '.pt')\n",
    "            \n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val {}: {:4f}'.format(SAVE_BEST, best_score))\n",
    "\n",
    "    return best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 150]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 150, 150]             128\n",
      "              ReLU-3         [-1, 64, 150, 150]               0\n",
      "         MaxPool2d-4           [-1, 64, 75, 75]               0\n",
      "            Conv2d-5           [-1, 64, 75, 75]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 75, 75]             128\n",
      "              ReLU-7           [-1, 64, 75, 75]               0\n",
      "            Conv2d-8           [-1, 64, 75, 75]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 75, 75]             128\n",
      "             ReLU-10           [-1, 64, 75, 75]               0\n",
      "           Conv2d-11          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 75, 75]             512\n",
      "           Conv2d-13          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 75, 75]             512\n",
      "             ReLU-15          [-1, 256, 75, 75]               0\n",
      "       Bottleneck-16          [-1, 256, 75, 75]               0\n",
      "           Conv2d-17           [-1, 64, 75, 75]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 75, 75]             128\n",
      "             ReLU-19           [-1, 64, 75, 75]               0\n",
      "           Conv2d-20           [-1, 64, 75, 75]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 75, 75]             128\n",
      "             ReLU-22           [-1, 64, 75, 75]               0\n",
      "           Conv2d-23          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 75, 75]             512\n",
      "             ReLU-25          [-1, 256, 75, 75]               0\n",
      "       Bottleneck-26          [-1, 256, 75, 75]               0\n",
      "           Conv2d-27           [-1, 64, 75, 75]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 75, 75]             128\n",
      "             ReLU-29           [-1, 64, 75, 75]               0\n",
      "           Conv2d-30           [-1, 64, 75, 75]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 75, 75]             128\n",
      "             ReLU-32           [-1, 64, 75, 75]               0\n",
      "           Conv2d-33          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 75, 75]             512\n",
      "             ReLU-35          [-1, 256, 75, 75]               0\n",
      "       Bottleneck-36          [-1, 256, 75, 75]               0\n",
      "           Conv2d-37          [-1, 128, 75, 75]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 75, 75]             256\n",
      "             ReLU-39          [-1, 128, 75, 75]               0\n",
      "           Conv2d-40          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 38, 38]             256\n",
      "             ReLU-42          [-1, 128, 38, 38]               0\n",
      "           Conv2d-43          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 38, 38]           1,024\n",
      "           Conv2d-45          [-1, 512, 38, 38]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-47          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-48          [-1, 512, 38, 38]               0\n",
      "           Conv2d-49          [-1, 128, 38, 38]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 38, 38]             256\n",
      "             ReLU-51          [-1, 128, 38, 38]               0\n",
      "           Conv2d-52          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 38, 38]             256\n",
      "             ReLU-54          [-1, 128, 38, 38]               0\n",
      "           Conv2d-55          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-57          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-58          [-1, 512, 38, 38]               0\n",
      "           Conv2d-59          [-1, 128, 38, 38]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 38, 38]             256\n",
      "             ReLU-61          [-1, 128, 38, 38]               0\n",
      "           Conv2d-62          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 38, 38]             256\n",
      "             ReLU-64          [-1, 128, 38, 38]               0\n",
      "           Conv2d-65          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-67          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-68          [-1, 512, 38, 38]               0\n",
      "           Conv2d-69          [-1, 128, 38, 38]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 38, 38]             256\n",
      "             ReLU-71          [-1, 128, 38, 38]               0\n",
      "           Conv2d-72          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 38, 38]             256\n",
      "             ReLU-74          [-1, 128, 38, 38]               0\n",
      "           Conv2d-75          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-77          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-78          [-1, 512, 38, 38]               0\n",
      "           Conv2d-79          [-1, 256, 38, 38]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 38, 38]             512\n",
      "             ReLU-81          [-1, 256, 38, 38]               0\n",
      "           Conv2d-82          [-1, 256, 19, 19]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 19, 19]             512\n",
      "             ReLU-84          [-1, 256, 19, 19]               0\n",
      "           Conv2d-85         [-1, 1024, 19, 19]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 19, 19]           2,048\n",
      "           Conv2d-87         [-1, 1024, 19, 19]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 19, 19]           2,048\n",
      "             ReLU-89         [-1, 1024, 19, 19]               0\n",
      "       Bottleneck-90         [-1, 1024, 19, 19]               0\n",
      "           Conv2d-91          [-1, 256, 19, 19]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 19, 19]             512\n",
      "             ReLU-93          [-1, 256, 19, 19]               0\n",
      "           Conv2d-94          [-1, 256, 19, 19]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 19, 19]             512\n",
      "             ReLU-96          [-1, 256, 19, 19]               0\n",
      "           Conv2d-97         [-1, 1024, 19, 19]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 19, 19]           2,048\n",
      "             ReLU-99         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-100         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-101          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 19, 19]             512\n",
      "            ReLU-103          [-1, 256, 19, 19]               0\n",
      "          Conv2d-104          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 19, 19]             512\n",
      "            ReLU-106          [-1, 256, 19, 19]               0\n",
      "          Conv2d-107         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-109         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-110         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-111          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 19, 19]             512\n",
      "            ReLU-113          [-1, 256, 19, 19]               0\n",
      "          Conv2d-114          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 19, 19]             512\n",
      "            ReLU-116          [-1, 256, 19, 19]               0\n",
      "          Conv2d-117         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-119         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-120         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-121          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 19, 19]             512\n",
      "            ReLU-123          [-1, 256, 19, 19]               0\n",
      "          Conv2d-124          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 19, 19]             512\n",
      "            ReLU-126          [-1, 256, 19, 19]               0\n",
      "          Conv2d-127         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-129         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-130         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-131          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 19, 19]             512\n",
      "            ReLU-133          [-1, 256, 19, 19]               0\n",
      "          Conv2d-134          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 19, 19]             512\n",
      "            ReLU-136          [-1, 256, 19, 19]               0\n",
      "          Conv2d-137         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-139         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-140         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-141          [-1, 512, 19, 19]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 19, 19]           1,024\n",
      "            ReLU-143          [-1, 512, 19, 19]               0\n",
      "          Conv2d-144          [-1, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-145          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-146          [-1, 512, 10, 10]               0\n",
      "          Conv2d-147         [-1, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-148         [-1, 2048, 10, 10]           4,096\n",
      "          Conv2d-149         [-1, 2048, 10, 10]       2,097,152\n",
      "     BatchNorm2d-150         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-151         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-152         [-1, 2048, 10, 10]               0\n",
      "          Conv2d-153          [-1, 512, 10, 10]       1,048,576\n",
      "     BatchNorm2d-154          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-155          [-1, 512, 10, 10]               0\n",
      "          Conv2d-156          [-1, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-157          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-158          [-1, 512, 10, 10]               0\n",
      "          Conv2d-159         [-1, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-160         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-161         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-162         [-1, 2048, 10, 10]               0\n",
      "          Conv2d-163          [-1, 512, 10, 10]       1,048,576\n",
      "     BatchNorm2d-164          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-165          [-1, 512, 10, 10]               0\n",
      "          Conv2d-166          [-1, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-167          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-168          [-1, 512, 10, 10]               0\n",
      "          Conv2d-169         [-1, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-170         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-171         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-172         [-1, 2048, 10, 10]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                    [-1, 1]           2,049\n",
      "================================================================\n",
      "Total params: 23,510,081\n",
      "Trainable params: 23,510,081\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 523.61\n",
      "Params size (MB): 89.68\n",
      "Estimated Total Size (MB): 614.32\n",
      "----------------------------------------------------------------\n",
      "resnet50_ResNet50NoneAmsGradAdam0005SmoothL1LossImgSize299SampleFrac0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 0/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.8649 KAPPA: 0.0037\n",
      "val Loss: 184265149.5385 KAPPA: 0.0009\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.8086 KAPPA: 0.0634\n",
      "val Loss: 28195.6762 KAPPA: 0.0004\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.5247 KAPPA: 0.0648\n",
      "val Loss: 851.0938 KAPPA: 0.0005\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.4401 KAPPA: 0.0501\n",
      "val Loss: 100.7268 KAPPA: 0.0001\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3915 KAPPA: 0.0845\n",
      "val Loss: 3.3388 KAPPA: -0.0176\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3862 KAPPA: -0.0375\n",
      "val Loss: 0.9711 KAPPA: -0.0726\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3493 KAPPA: 0.1303\n",
      "val Loss: 0.7777 KAPPA: 0.0064\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3776 KAPPA: 0.1296\n",
      "val Loss: 0.7586 KAPPA: -0.0026\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3432 KAPPA: 0.1461\n",
      "val Loss: 0.5949 KAPPA: -0.0940\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3359 KAPPA: 0.2260\n",
      "val Loss: 0.5770 KAPPA: 0.0580\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Training complete in 2m 15s\n",
      "Best val loss: 0.576951\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 150]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 150, 150]             128\n",
      "              ReLU-3         [-1, 64, 150, 150]               0\n",
      "         MaxPool2d-4           [-1, 64, 75, 75]               0\n",
      "            Conv2d-5           [-1, 64, 75, 75]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 75, 75]             128\n",
      "              ReLU-7           [-1, 64, 75, 75]               0\n",
      "            Conv2d-8           [-1, 64, 75, 75]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 75, 75]             128\n",
      "             ReLU-10           [-1, 64, 75, 75]               0\n",
      "           Conv2d-11          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 75, 75]             512\n",
      "           Conv2d-13          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 75, 75]             512\n",
      "             ReLU-15          [-1, 256, 75, 75]               0\n",
      "       Bottleneck-16          [-1, 256, 75, 75]               0\n",
      "           Conv2d-17           [-1, 64, 75, 75]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 75, 75]             128\n",
      "             ReLU-19           [-1, 64, 75, 75]               0\n",
      "           Conv2d-20           [-1, 64, 75, 75]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 75, 75]             128\n",
      "             ReLU-22           [-1, 64, 75, 75]               0\n",
      "           Conv2d-23          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 75, 75]             512\n",
      "             ReLU-25          [-1, 256, 75, 75]               0\n",
      "       Bottleneck-26          [-1, 256, 75, 75]               0\n",
      "           Conv2d-27           [-1, 64, 75, 75]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 75, 75]             128\n",
      "             ReLU-29           [-1, 64, 75, 75]               0\n",
      "           Conv2d-30           [-1, 64, 75, 75]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 75, 75]             128\n",
      "             ReLU-32           [-1, 64, 75, 75]               0\n",
      "           Conv2d-33          [-1, 256, 75, 75]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 75, 75]             512\n",
      "             ReLU-35          [-1, 256, 75, 75]               0\n",
      "       Bottleneck-36          [-1, 256, 75, 75]               0\n",
      "           Conv2d-37          [-1, 128, 75, 75]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 75, 75]             256\n",
      "             ReLU-39          [-1, 128, 75, 75]               0\n",
      "           Conv2d-40          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 38, 38]             256\n",
      "             ReLU-42          [-1, 128, 38, 38]               0\n",
      "           Conv2d-43          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 38, 38]           1,024\n",
      "           Conv2d-45          [-1, 512, 38, 38]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-47          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-48          [-1, 512, 38, 38]               0\n",
      "           Conv2d-49          [-1, 128, 38, 38]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 38, 38]             256\n",
      "             ReLU-51          [-1, 128, 38, 38]               0\n",
      "           Conv2d-52          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 38, 38]             256\n",
      "             ReLU-54          [-1, 128, 38, 38]               0\n",
      "           Conv2d-55          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-57          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-58          [-1, 512, 38, 38]               0\n",
      "           Conv2d-59          [-1, 128, 38, 38]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 38, 38]             256\n",
      "             ReLU-61          [-1, 128, 38, 38]               0\n",
      "           Conv2d-62          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 38, 38]             256\n",
      "             ReLU-64          [-1, 128, 38, 38]               0\n",
      "           Conv2d-65          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-67          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-68          [-1, 512, 38, 38]               0\n",
      "           Conv2d-69          [-1, 128, 38, 38]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 38, 38]             256\n",
      "             ReLU-71          [-1, 128, 38, 38]               0\n",
      "           Conv2d-72          [-1, 128, 38, 38]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 38, 38]             256\n",
      "             ReLU-74          [-1, 128, 38, 38]               0\n",
      "           Conv2d-75          [-1, 512, 38, 38]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 38, 38]           1,024\n",
      "             ReLU-77          [-1, 512, 38, 38]               0\n",
      "       Bottleneck-78          [-1, 512, 38, 38]               0\n",
      "           Conv2d-79          [-1, 256, 38, 38]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 38, 38]             512\n",
      "             ReLU-81          [-1, 256, 38, 38]               0\n",
      "           Conv2d-82          [-1, 256, 19, 19]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 19, 19]             512\n",
      "             ReLU-84          [-1, 256, 19, 19]               0\n",
      "           Conv2d-85         [-1, 1024, 19, 19]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 19, 19]           2,048\n",
      "           Conv2d-87         [-1, 1024, 19, 19]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 19, 19]           2,048\n",
      "             ReLU-89         [-1, 1024, 19, 19]               0\n",
      "       Bottleneck-90         [-1, 1024, 19, 19]               0\n",
      "           Conv2d-91          [-1, 256, 19, 19]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 19, 19]             512\n",
      "             ReLU-93          [-1, 256, 19, 19]               0\n",
      "           Conv2d-94          [-1, 256, 19, 19]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 19, 19]             512\n",
      "             ReLU-96          [-1, 256, 19, 19]               0\n",
      "           Conv2d-97         [-1, 1024, 19, 19]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 19, 19]           2,048\n",
      "             ReLU-99         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-100         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-101          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 19, 19]             512\n",
      "            ReLU-103          [-1, 256, 19, 19]               0\n",
      "          Conv2d-104          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 19, 19]             512\n",
      "            ReLU-106          [-1, 256, 19, 19]               0\n",
      "          Conv2d-107         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-109         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-110         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-111          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 19, 19]             512\n",
      "            ReLU-113          [-1, 256, 19, 19]               0\n",
      "          Conv2d-114          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 19, 19]             512\n",
      "            ReLU-116          [-1, 256, 19, 19]               0\n",
      "          Conv2d-117         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-119         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-120         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-121          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 19, 19]             512\n",
      "            ReLU-123          [-1, 256, 19, 19]               0\n",
      "          Conv2d-124          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 19, 19]             512\n",
      "            ReLU-126          [-1, 256, 19, 19]               0\n",
      "          Conv2d-127         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-129         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-130         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-131          [-1, 256, 19, 19]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 19, 19]             512\n",
      "            ReLU-133          [-1, 256, 19, 19]               0\n",
      "          Conv2d-134          [-1, 256, 19, 19]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 19, 19]             512\n",
      "            ReLU-136          [-1, 256, 19, 19]               0\n",
      "          Conv2d-137         [-1, 1024, 19, 19]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 19, 19]           2,048\n",
      "            ReLU-139         [-1, 1024, 19, 19]               0\n",
      "      Bottleneck-140         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-141          [-1, 512, 19, 19]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 19, 19]           1,024\n",
      "            ReLU-143          [-1, 512, 19, 19]               0\n",
      "          Conv2d-144          [-1, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-145          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-146          [-1, 512, 10, 10]               0\n",
      "          Conv2d-147         [-1, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-148         [-1, 2048, 10, 10]           4,096\n",
      "          Conv2d-149         [-1, 2048, 10, 10]       2,097,152\n",
      "     BatchNorm2d-150         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-151         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-152         [-1, 2048, 10, 10]               0\n",
      "          Conv2d-153          [-1, 512, 10, 10]       1,048,576\n",
      "     BatchNorm2d-154          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-155          [-1, 512, 10, 10]               0\n",
      "          Conv2d-156          [-1, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-157          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-158          [-1, 512, 10, 10]               0\n",
      "          Conv2d-159         [-1, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-160         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-161         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-162         [-1, 2048, 10, 10]               0\n",
      "          Conv2d-163          [-1, 512, 10, 10]       1,048,576\n",
      "     BatchNorm2d-164          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-165          [-1, 512, 10, 10]               0\n",
      "          Conv2d-166          [-1, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-167          [-1, 512, 10, 10]           1,024\n",
      "            ReLU-168          [-1, 512, 10, 10]               0\n",
      "          Conv2d-169         [-1, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-170         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-171         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-172         [-1, 2048, 10, 10]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                    [-1, 1]           2,049\n",
      "================================================================\n",
      "Total params: 23,510,081\n",
      "Trainable params: 23,510,081\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 523.61\n",
      "Params size (MB): 89.68\n",
      "Estimated Total Size (MB): 614.32\n",
      "----------------------------------------------------------------\n",
      "resnet50_ResNet50ReduceLROnPlateauAmsGradAdam0005SmoothL1LossImgSize299SampleFrac0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 0/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3342 KAPPA: 0.1520\n",
      "val Loss: 0.5441 KAPPA: -0.0150\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3256 KAPPA: 0.1408\n",
      "val Loss: 0.5559 KAPPA: -0.0514\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3234 KAPPA: 0.1830\n",
      "val Loss: 0.6118 KAPPA: -0.1637\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3286 KAPPA: 0.1781\n",
      "val Loss: 0.4982 KAPPA: 0.0000\n",
      "Saving the best model at /mnt/diabetic_retinopathy_v2/models\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3096 KAPPA: 0.2392\n",
      "val Loss: 0.5782 KAPPA: -0.1191\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3189 KAPPA: 0.2822\n",
      "val Loss: 0.5661 KAPPA: -0.1137\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3117 KAPPA: 0.1950\n",
      "val Loss: 0.5344 KAPPA: -0.1353\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.2989 KAPPA: 0.2143\n",
      "val Loss: 0.5546 KAPPA: -0.0591\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.2940 KAPPA: 0.3039\n",
      "val Loss: 0.6064 KAPPA: -0.1320\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Learning Rate: 0.0005\n",
      "train Loss: 0.3272 KAPPA: 0.2489\n",
      "val Loss: 0.5148 KAPPA: -0.1196\n",
      "\n",
      "Training complete in 2m 15s\n",
      "Best val loss: 0.498201\n"
     ]
    }
   ],
   "source": [
    "model_name_list = []\n",
    "metric_list = []\n",
    "\n",
    "for m in MODELS:\n",
    "    \n",
    "    model_parameters = NET_LIST[m]\n",
    "    base_model = model_parameters['base_model']\n",
    "    model = getModel(m, NUM_CLASSES)\n",
    "    \n",
    "    for o in OPTIMIZERS:\n",
    "            \n",
    "        optimizer = getOptimizer(o, model)\n",
    "        \n",
    "        for s in SCHEDULERS:\n",
    "    \n",
    "            scheduler = getScheduler(s, optimizer)\n",
    "            \n",
    "            for l in LOSSES:\n",
    "\n",
    "                criterion = getLossFunction(l)\n",
    "\n",
    "                model_name = f'{base_model}_{m}{s}{o}{l}ImgSize{str(INPUT_SIZE)}SampleFrac{SAMPLE_FRAC}'\n",
    "\n",
    "                tensorboard = SummaryWriter(comment = model_name)\n",
    "\n",
    "                summary(model, input_size=(CHANNELS, INPUT_SIZE, INPUT_SIZE))\n",
    "                \n",
    "                # Train and evaluate\n",
    "                best_score = train_model(\n",
    "                    model, \n",
    "                    model_name, \n",
    "                    l,\n",
    "                    dataloaders_dict, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    num_epochs=NUM_EPOCH, \n",
    "                    is_inception=NET_LIST[m]['is_inception'])\n",
    "\n",
    "                model_name_list.append(model_name)\n",
    "                metric_list.append(best_score)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Best Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()    \n",
    "width = 0.75 # the width of the bars \n",
    "ind = np.arange(len(metric_list))  # the x locations for the groups\n",
    "ax.barh(ind, metric_list, width)\n",
    "ax.set_yticks(ind+width/2)\n",
    "ax.set_yticklabels(model_name_list, minor=False)\n",
    "plt.xlabel('Loss')\n",
    "for i, v in enumerate(metric_list):\n",
    "    ax.text(v, i, str(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
